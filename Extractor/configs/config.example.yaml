# Example configuration for the extraction pipeline
llm:
  provider: ollama  # ollama (local default) | stub (offline) | bedrock_claude | chatgpt | anthropic
  model_id: mistral:latest
  region: us-east-2  # used for bedrock_claude
  temperature: 0.1
  max_tokens: 256
  top_k: 250
  retries: 0
  backoff: 1.5

parallel:
  enabled: false
  num_workers: null

regularization:
  ocr_confidence_threshold: 0.8
  max_pages: 500

merge:
  similarity_threshold: 0.9

validation:
  confidence_threshold: 0.7
  flag_issue_rate: 0.2

stage5:
  generate: true
  ingest: true

metadata_resolver:
  use_regex: true
  tenant_owner_default: null
  tenant_effective_date_default: null
  tenant_regulatory_linkage_default: []
  domain_defaults: {}

scope:
  fallback: all  # all | unknown | none
  enable_regex: true

double_run:
  enabled: false

docai:
  project_id: your-gcp-project-id
  location: us  # e.g., us, eu
  processor_id: your-processor-id
  processor_version: null  # optional; null uses the default processor version
